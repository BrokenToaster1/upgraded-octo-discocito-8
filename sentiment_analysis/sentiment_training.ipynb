{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code block imports the glove embeddings.\n",
    "#It can take anywhere from 15 to 100 seconds depending on your computer.\n",
    "\n",
    "path = \"glove.6B.50d.txt.w2v\"\n",
    "t0 = time.time()\n",
    "glove = KeyedVectors.load_word2vec_format(path, binary=False)\n",
    "t1 = time.time()\n",
    "print(\"elapsed %ss\" % (t1 - t0))\n",
    "# 50d: elapsed 17.67420792579651s\n",
    "# 100d: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line will throw an error if there was a failure in the glove embeddings.\n",
    "#Checks if a word is in the glove. Should always print True.\n",
    "'meme' in glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opens the test data and allows you to see how it's formatted before the processing.\n",
    "#THIS CELL DOES NOT NEED TO BE RUN.\n",
    "\n",
    "play_with_test_data = np.load('./data/test_twitter_data.npz')\n",
    "play_with_test_data = play_with_test_data.f.arr_0\n",
    "play_with_test_data[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports the training and testing data and formats them into usable arrays.\n",
    "\n",
    "from preprocess import new_preprocess\n",
    "\n",
    "x_test, test_max = new_preprocess(\"./data/test_twitter_data.npz\")\n",
    "x_train, train_max = new_preprocess(\"./data/train_twitter_data.npz\")\n",
    "\n",
    "with np.load(\"./data/train_twitter_label.npz\") as f:\n",
    "    y_train = f[\"arr_0\"].astype(int)\n",
    "\n",
    "with np.load(\"./data/test_twitter_label.npz\") as f:\n",
    "    y_test = f[\"arr_0\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Allows you to see how the data is currently formatted\n",
    "\n",
    "print(x_train[0])\n",
    "print('\\n', y_train[0])\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_input = 50, dim_recurrent = 100, dim_output = 2):\n",
    "        \n",
    "        '''\n",
    "        Initializes the model.\n",
    "        \n",
    "        INPUTS:\n",
    "            dim_input - The dimensionality of the input data.\n",
    "                Defaults to 50, the size of word embeddings.\n",
    "            dim_recurrent - The number of recurrent layers.\n",
    "                This is a hyperparameter. Defaults to 100.\n",
    "            dim_output - The number of predictions to make.\n",
    "                Defaults to 2, the number of predictions the model should make.\n",
    "                \n",
    "        OUTPUTS:\n",
    "            None\n",
    "        '''\n",
    "        #Initializes model as a pytorch object\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        #Initializes internal variables\n",
    "        self.C = dim_input\n",
    "        self.D = dim_recurrent\n",
    "        self.K = dim_output\n",
    "        \n",
    "        \n",
    "        #Initializes the internal layers of the network.\n",
    "        self.dense1 = nn.Linear(dim_input, dim_recurrent)\n",
    "        self.dense2 = nn.Linear(dim_recurrent, dim_recurrent, bias = False)\n",
    "        self.dense3 = nn.Linear(dim_input, dim_recurrent)\n",
    "        self.dense4 = nn.Linear(dim_recurrent, dim_recurrent, bias = False)\n",
    "        self.dense5 = nn.Linear(dim_recurrent, dim_output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        '''\n",
    "        Takes in a batch of N tweets and outputs N predictions from an RNN.\n",
    "        \n",
    "        INPUT:\n",
    "            x - batch of tweets to be processed.\n",
    "            \n",
    "        OUTPUT:\n",
    "            predictions - predictions for each tweet.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Creates the hidden layer\n",
    "        hidden = torch.zeros(len(x), self.D)\n",
    "        \n",
    "        #Processes each row\n",
    "        for i in range(x.shape[2]):\n",
    "            row = x[:, :, i]\n",
    "            \n",
    "            #Iterates through the RNN\n",
    "            subHid = self.dense1(row)\n",
    "            mem = self.dense2(hidden)\n",
    "            subHid += mem\n",
    "            subHid = F.relu(subHid)\n",
    "            z = F.sigmoid(self.dense3(row) + self.dense4(hidden))\n",
    "            hidden = z * hidden + (1 - z) * subHid\n",
    "        \n",
    "        #Converts the final hidden state to predictions.\n",
    "        return self.dense5(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(tweets):\n",
    "    '''\n",
    "    Takes in a batch of tweets and formats them for training.\n",
    "    \n",
    "    INPUT:\n",
    "        tweets - batch of tweets to process.\n",
    "        \n",
    "    OUTPUT:\n",
    "        ret - processed tweets as word embeddings ready for the RNN.\n",
    "    '''\n",
    "    ret = torch.zeros((len(tweets), len(max(tweets, key = len)), 50))\n",
    "    for n in range(len(tweets)):\n",
    "        tweet = tweets[n]\n",
    "        for x in range(len(tweet)):\n",
    "            word = tweet[x]\n",
    "            if word in glove:\n",
    "                ret[n, x] = torch.tensor(glove[word])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, truth):\n",
    "    '''\n",
    "    Gets the accuracy of predictions when compared to truth data.\n",
    "    '''\n",
    "    maxes = torch.argmax(pred, dim = -1)\n",
    "    maxes = maxes == truth\n",
    "    maxes = maxes.type(torch.FloatTensor)\n",
    "    return torch.mean(maxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializes the network.\n",
    "\n",
    "net = Model()\n",
    "from torch.optim import Adam\n",
    "\n",
    "optim = Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a cool liveplot!\n",
    "\n",
    "%matplotlib notebook\n",
    "import liveplot\n",
    "plotter, fig, ax = liveplot.create_plot(metrics=[\"loss\", \"accuracy\"], refresh=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code saves the model, then reopens it as a newNet.\n",
    "#Does not run when shift-tabbing through the data due to the liveplot.\n",
    "\n",
    "from pickle import dump, load\n",
    "dump(net, open('sentnet.dat', 'wb'))\n",
    "\n",
    "newNet = load(open('sentnet.dat', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "soft = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch_cnt in range(10):\n",
    "    \n",
    "    idxs = np.arange(len(x_train))\n",
    "    np.random.shuffle(idxs)\n",
    "    \n",
    "    for batch_cnt in range(len(x_train) // batch_size):\n",
    "        \n",
    "        batch = [x_train[i] for i in idxs[batch_cnt * batch_size : (batch_cnt + 1) * batch_size]]\n",
    "        \n",
    "        batch = process(batch)\n",
    "        \n",
    "        batch = torch.transpose(batch, 1, 2)\n",
    "        \n",
    "        prediction = net(batch)\n",
    "        \n",
    "        truth = torch.LongTensor([y_train[i] for i in idxs[batch_cnt * batch_size : (batch_cnt + 1) * batch_size]])\n",
    "        \n",
    "        loss = soft(prediction, truth)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()\n",
    "        \n",
    "        acc = accuracy(prediction, truth)\n",
    "        \n",
    "        plotter.set_train_batch({\"loss\" : loss.item(),\n",
    "                                 \"accuracy\" : acc.item()},\n",
    "                                 batch_size=batch_size)\n",
    "    with torch.no_grad():\n",
    "        idxs = np.arange(len(x_test))\n",
    "        idxs = np.random.shuffle(idxs)\n",
    "        for batch_cnt in range(0, len(x_test)//batch_size):\n",
    "            batch_indices = slice(batch_cnt*batch_size, (batch_cnt + 1)*batch_size) # make as slice\n",
    "            batch = x_test[batch_indices]\n",
    "\n",
    "            batch = process(batch)\n",
    "\n",
    "            batch = torch.transpose(batch, 1, 2)\n",
    "\n",
    "\n",
    "\n",
    "            prediction = net(batch)\n",
    "\n",
    "            truth = torch.LongTensor(y_test[batch_indices])\n",
    "\n",
    "            loss = soft(prediction, truth)\n",
    "\n",
    "            acc = accuracy(prediction, truth)\n",
    "\n",
    "            plotter.set_test_batch({\"loss\" : loss.item(),\n",
    "                                     \"accuracy\" : acc.item()},\n",
    "                                     batch_size=batch_size)\n",
    "    plotter.plot_train_epoch()\n",
    "    plotter.plot_test_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "\n",
    "def sentiment(sentence, path = 'sentnet.dat'):\n",
    "    '''\n",
    "    Decides whether or not a string has happy sentiment.\n",
    "    \n",
    "    INPUTS:\n",
    "        sentence - string to be analyzed\n",
    "        path (optional) - string with the path to the databse\n",
    "        \n",
    "    OUTPUT:\n",
    "        sentiment (int) - 0 if the sentiment is negative, 1 if it's positive.\n",
    "    '''\n",
    "    \n",
    "    net = load(open(path, 'rb'))\n",
    "    sentence = process([sentence.split()])\n",
    "    sentence = torch.transpose(sentence, 1, 2)\n",
    "    return torch.argmax(net(sentence)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It knows that loving my mom is good! Woo-hoo!\n",
    "sentiment('I love my mom')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
