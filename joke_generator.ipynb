{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(counter):\n",
    "    \"\"\" Converts a letter -> count counter to a list of (letter, \n",
    "    frequency) pairs, sorted in descending order of frequency.\n",
    "    \n",
    "        Parameters\n",
    "        -----------\n",
    "        counter : collections.Counter\n",
    "            letter -> count\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        A list of (letter, frequency) pairs, sorted in descending \n",
    "        order of frequency. \"\"\"\n",
    "\n",
    "    total = sum(counter.values())\n",
    "    return [(char, cnt/total) for char, cnt in counter.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lm(text, n):\n",
    "    \"\"\" Trains a character-based n-gram language model.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        text: str \n",
    "            \n",
    "        n: int\n",
    "            the length of the n-gram to analyze.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        A dictionary that maps history to a list of tuples that \n",
    "        describes the probability of each following character. \"\"\"\n",
    "    \n",
    "    raw_lm = defaultdict(Counter)\n",
    "    history = text[:n]\n",
    "    \n",
    "    for char in text[n:]:\n",
    "        raw_lm[history][char] += 1\n",
    "        history = history[1:] + char\n",
    "    \n",
    "    lm = {history : normalize(counter) for history, counter in raw_lm.items()}\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_letter(lm, history):\n",
    "    \"\"\" Randomly generates a letter according to the probability \n",
    "    distribution associated with the specified history.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lm: Dict[str, List[Tuple[str, float]]] \n",
    "            the n-gram language model. \n",
    "        \n",
    "        history: str\n",
    "            a string of length (n-1) to use as history when generating \n",
    "            the next character.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        The predicted character. \"\"\"\n",
    "    \n",
    "    if not history in lm:\n",
    "        return chr(np.random.randint(97, 97 + 26))\n",
    "    letters, probs = tuple(zip(*lm[history]))\n",
    "    i = np.random.choice(letters, p=probs)\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phrase(lm, n, total_words = 1):\n",
    "    \"\"\" Randomly generates a phrase by drawing from the probability \n",
    "    distributions stored in the n-gram language model.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        lm: Dict[str, List[Tuple[str, float]]]\n",
    "            the n-gram language model. \n",
    "            \n",
    "        n: int\n",
    "            order of n-gram model.\n",
    "            \n",
    "        total_words : int\n",
    "            the number of words to be generated\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Model-generated phrase. \"\"\"\n",
    "\n",
    "    word_start_hist = [hist for hist in lm.keys() if hist.startswith(' ')]\n",
    "    i = np.random.randint(len(word_start_hist))\n",
    "    history = word_start_hist[i]\n",
    "    \n",
    "    text = []\n",
    "    text.extend(history[1:])\n",
    "    \n",
    "    spaces = 0\n",
    "    \n",
    "    while True:\n",
    "        c = generate_letter(lm, history)\n",
    "        if c == ' ':\n",
    "            spaces += 1\n",
    "            if spaces == total_words:\n",
    "                break\n",
    "        text.append(c)\n",
    "        history = history[1:] + c\n",
    "        \n",
    "    return \"\".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_jokes(path_to_nouns, path_to_verbs, n):\n",
    "    \"\"\" Generates a really funny joke based on a text file of words.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        path_to_words : string\n",
    "            a string with the path to the text file of words to be\n",
    "            including in the jokes\n",
    "            \n",
    "        n : scalar\n",
    "            the value of n for the n-gram model\n",
    "                                \n",
    "        Returns\n",
    "        -------\n",
    "        A really funny joke. \"\"\"\n",
    "\n",
    "    with open(path_to_nouns, \"r\") as f:\n",
    "        nouns = f.read()\n",
    "    nouns = \" \".join(nouns.split())\n",
    "    \n",
    "    with open(path_to_verbs, \"r\") as f:\n",
    "        verbs = f.read()\n",
    "    verbs = \" \".join(verbs.split())\n",
    "    \n",
    "    lm_noun = train_lm(nouns, n)\n",
    "    lm_verb = train_lm(verbs, n)\n",
    "        \n",
    "    jokes = [\"Knock knock. \\nWho's there? \\n{0} \\n{0} who? \\n{0} {1} \".format(generate_phrase(lm_noun, n, np.random.randint(1, 3)).capitalize(), generate_phrase(lm_noun, n, np.random.randint(1, 3))),                \n",
    "             \"Why did the {} {} the {}? \\nTo {} {}!\".format(generate_phrase(lm_noun, n, np.random.randint(1, 3)), generate_phrase(lm_verb, n), generate_phrase(lm_noun, n, np.random.randint(1, 3)), generate_phrase(lm_verb, n), generate_phrase(lm_noun, n, np.random.randint(1, 3))),\n",
    "             \"How many {} does it take to {} a {}? \\n{}\".format(generate_phrase(lm_noun, n, np.random.randint(1, 3)), generate_phrase(lm_verb, n), generate_phrase(lm_noun, n, np.random.randint(1, 3)), generate_phrase(lm_noun, n).capitalize()),\n",
    "             \"*slaps roof of {0}* \\nThis {0} can fit so much {1} in it\".format(generate_phrase(lm_noun, n), generate_phrase(lm_noun, n)),\n",
    "             \"Thank you {} very {}\".format(generate_phrase(lm_noun, n, np.random.randint(1, 3)), generate_phrase(lm_noun, n, np.random.randint(1, 3)))]    \n",
    "    i = np.random.randint(len(jokes))    \n",
    "    return jokes[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knock knock. \n",
      "Who's there? \n",
      "Necessary \n",
      "Necessary who? \n",
      "Necessary death \n"
     ]
    }
   ],
   "source": [
    "print(n_gram_jokes(\"nouns.txt\", \"verbs.txt\", 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
