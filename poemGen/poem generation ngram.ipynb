{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from pickle import load, dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def globData(root = 'data'):\n",
    "    \n",
    "    '''\n",
    "    Collects text files from three folders and returns them as a list.\n",
    "    INPUT:\n",
    "        root (optional): the folder containing the poem folders.\n",
    "        \n",
    "    OUTPUT:\n",
    "        ret: 3-tuple with (normal, happy, sad) data.\n",
    "        Each object is a list. Each list item is one text file found in the folders.\n",
    "    '''\n",
    "    \n",
    "    root = Path(root)\n",
    "    \n",
    "    assert root.exists(), 'Path input does not lead anywhere.'\n",
    "    \n",
    "    ret = [[], [], []]\n",
    "    \n",
    "    for file in (root / 'regularpoems').glob('*.txt'):\n",
    "        \n",
    "        with open(file, mode = 'rb') as f:\n",
    "            ret[0].append(f.read().decode(errors = 'ignore'))\n",
    "        \n",
    "    for file in (root / 'happypoems').glob('*.txt'):\n",
    "        \n",
    "        with open(file, mode = 'rb') as f:\n",
    "            ret[1].append(f.read().decode(errors = 'ignore'))\n",
    "        \n",
    "    for file in (root / 'sadpoems').glob('*.txt'):\n",
    "        \n",
    "        with open(file, mode = 'rb') as f:\n",
    "            ret[2].append(f.read().decode(errors = 'ignore'))\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm, happy, sad = globData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(counter):\n",
    "    \"\"\" Convert a `letter -> count` counter to a list \n",
    "       of (letter, frequency) pairs, sorted in descending order of \n",
    "       frequency.\n",
    "    \n",
    "        Parameters\n",
    "        -----------\n",
    "        counter : collections.Counter\n",
    "            letter -> count\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        List[Tuple[str, int]]\n",
    "           A list of tuples - (letter, frequency) pairs in order\n",
    "           of descending-frequency\n",
    "        \n",
    "        Examples\n",
    "        --------\n",
    "        >>> from collections import Counter\n",
    "        >>> letter_count = Counter({\"a\": 1, \"b\": 3})\n",
    "        >>> letter_count\n",
    "        Counter({'a': 1, 'b': 3})\n",
    "        \n",
    "        >>> normalize(letter_count)\n",
    "        [('b', 0.75), ('a', 0.25)]\n",
    "    \"\"\"\n",
    "    total = sum(counter.values())\n",
    "    return [(char, cnt/total) for char, cnt in counter.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lm(texts, n):\n",
    "    \"\"\" Train character-based n-gram language model.\n",
    "        \n",
    "        This will learn: given a sequence of n-1 characters, what the probability\n",
    "        distribution is for the n-th character in the sequence.\n",
    "        \n",
    "        For example if we train on the text:\n",
    "            text = \"cacao\"\n",
    "        \n",
    "        Using a n-gram size of n=3, then the following dict would be returned.\n",
    "        See that we *normalize* each of the counts for a given history\n",
    "        \n",
    "            {'ac': [('a', 1.0)],\n",
    "             'ca': [('c', 0.5), ('o', 0.5)],\n",
    "             '~c': [('a', 1.0)],\n",
    "             '~~': [('c', 1.0)]}\n",
    "\n",
    "        Tildas (\"~\") are used for padding the history when necessary, so that it's \n",
    "        possible to estimate the probability of a seeing a character when there \n",
    "        aren't (n - 1) previous characters of history available.\n",
    "        \n",
    "        So, according to this text we trained on, if you see the sequence 'ac',\n",
    "        our model predicts that the next character should be 'a' 100% of the time.\n",
    "        \n",
    "       For generatiing the padding, recall that Python allows you to generate \n",
    "        repeated sequences easily: \n",
    "           `\"p\" * 4` returns `\"pppp\"`\n",
    "           \n",
    "        Parameters\n",
    "        -----------\n",
    "        text: str \n",
    "            A string (doesn't need to be lowercased).\n",
    "        n: int\n",
    "            The length of n-gram to analyze.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, List[Tuple[str, float]]]\n",
    "          {n-1 history -> [(letter, normalized count), ...]}\n",
    "        A dict that maps histories (strings of length (n-1)) to lists of (char, prob) \n",
    "        pairs, where prob is the probability (i.e frequency) of char appearing after \n",
    "        that specific history.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> train_lm(\"cacao\", 3)\n",
    "        {'ac': [('a', 1.0)],\n",
    "         'ca': [('c', 0.5), ('o', 0.5)],\n",
    "         '~c': [('a', 1.0)],\n",
    "         '~~': [('c', 1.0)]}\n",
    "    \"\"\"\n",
    "    raw_lm = defaultdict(Counter)\n",
    "    \n",
    "    # count number of times characters appear following different histories\n",
    "    # `raw_lm`: {history -> Counter}\n",
    "    for text in texts:\n",
    "        history = \"~\" * (n - 1)\n",
    "        for char in text:\n",
    "            raw_lm[history][char] += 1\n",
    "            # slide history window to the right by one character\n",
    "            history = history[1:] + char\n",
    "    \n",
    "    # create final dictionary, normalizing the counts for each history\n",
    "    lm = {history : normalize(counter) for history, counter in raw_lm.items()}\n",
    "    \n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(pairs):\n",
    "    \"\"\"\n",
    "    \"unzips\" of groups of items into separate lists.\n",
    "    \n",
    "    Example: pairs = [(\"a\", 1), (\"b\", 2), ...] --> ((\"a\", \"b\", ...), (1, 2, ...))\n",
    "    \"\"\"\n",
    "    return tuple(zip(*pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_letter(lm, history):\n",
    "    \"\"\" Randomly picks letter according to probability distribution associated with \n",
    "        the specified history, as stored in your language model.\n",
    "    \n",
    "        Note: returns dummy character \"~\" if history not found in model.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        lm: Dict[str, List[Tuple[str, float]]] \n",
    "            The n-gram language model. \n",
    "            I.e. the dictionary: history -> [(char, freq), ...]\n",
    "        \n",
    "        history: str\n",
    "            A string of length (n-1) to use as context/history for generating \n",
    "            the next character.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The predicted character. '~' if history is not in language model.\n",
    "    \"\"\"\n",
    "    if not history in lm:\n",
    "        return \"~\"\n",
    "    letters, probs = unzip(lm[history])\n",
    "    i = np.random.choice(letters, p=probs)\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(lm, n, nletters=100):\n",
    "    \"\"\" Randomly generates `nletters` of text by drawing from \n",
    "        the probability distributions stored in a n-gram language model \n",
    "        `lm`.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        lm: Dict[str, List[Tuple[str, float]]]\n",
    "            The n-gram language model. \n",
    "            I.e. the dictionary: history -> [(char, freq), ...]\n",
    "        n: int\n",
    "            Order of n-gram model.\n",
    "        nletters: int\n",
    "            Number of letters to randomly generate.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Model-generated text.\n",
    "    \"\"\"\n",
    "    history = \"~\" * (n - 1)\n",
    "    text = []\n",
    "    for i in range(nletters):\n",
    "        c = generate_letter(lm, history)\n",
    "        text.append(c)\n",
    "        history = history[1:] + c\n",
    "    return \"\".join(text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adds the normal (base) data to the happy and sad bases.\n",
    "happy += norm\n",
    "sad += norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trains and saves the model.\n",
    "happy_lm = train_lm(happy, N)\n",
    "dump(happy_lm, open('happy.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trains the model and saves the model.\n",
    "sad_lm = train_lm(sad, N)\n",
    "dump(sad_lm, open('sad.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter I \r\n",
      "\r\n",
      "\r\n",
      "AN UNEXPECTED PARTY \r\n",
      "\r\n",
      "\r\n",
      "In a different at first seem. Certainly not going to.\"\r\n",
      "\"No.\"\r\n",
      "\"I see.\"\r\n",
      "\r\n",
      "\"My family all died and I came into this dive \r\n",
      "you get held in this sort of questions here,\" he said, \r\n",
      "\"I've got great news! I've located the ship's artificial night closed in they were after him. Even temporarily refracted \r\n",
      "into a tense crouch, feeling for the moment floating soggily on the \r\n",
      "drawing-room and thought. The word yellow wandered through the dark rustling\r\n",
      "trees. \n"
     ]
    }
   ],
   "source": [
    "#See it work!\n",
    "print(generate_text(happy_lm, N, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Angel of the Odd,when this hog, I say, which hitherto had\r\n",
      "been driving a little, things for myself only, and trying to find out what they had seen his parents were brave.... I killed your father gave me no clew in this respect, in as good order at the expiration of an hour, the fair and debonair, that now so lowly lies,\r\n",
      "The life upon her yellow hair but not without some claim to attention, and get me to come\r\n",
      "from over the hill, and there was a knock. \r\n",
      "\r\n",
      "\"Hello,\" said Harry, trying to hi\n"
     ]
    }
   ],
   "source": [
    "#See it work!\n",
    "print(generate_text(sad_lm, N, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿Far out in the end, not quite sure neither of which tended to make breakfast. \r\n",
      "\r\n",
      "They all nodded. A favorite game in quarry had been quite expensive. \r\n",
      "\r\n",
      "\"I mean it,\" he added.\r\n",
      "\"You know sometimes politer in word than in deed. The time would come out of it, first on one side. \"Right, then,\" said Adam, who had accepted his\r\n",
      "hospitality and so become authorities on his past, had increased\r\n",
      "her value in his eyes. Now he felt \r\n",
      "he ought to first sort out,\" said Arthur.\n"
     ]
    }
   ],
   "source": [
    "def genHappy(letters = 500, N = 13):\n",
    "    '''\n",
    "    Generates a short, happy story.\n",
    "    \n",
    "    INPUTS:\n",
    "        letters (optional): the numbers of letters to generate before processing.\n",
    "        NOTE: This argument does not necessarily guarantee the length of the story.\n",
    "        However, it will generally be close to this.\n",
    "        N (optional): the N value used in training the data.\n",
    "        \n",
    "    OUTPUT:\n",
    "        text: a string containing the happy story.\n",
    "    '''\n",
    "    text = generate_text(happy_lm, N, letters)\n",
    "    if '.' in text:\n",
    "        while text[-1] != '.':\n",
    "            text = text[:-1]\n",
    "    return text\n",
    "print(genHappy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAPTER ONE \r\n",
      "\r\n",
      "THE BOY WHO LIVED \r\n",
      "\r\n",
      "Mr. and Mrs. Dursley on the cheek, and her hand was still very \r\n",
      "skilled. \r\n",
      "\r\n",
      "As they stood, the\r\n",
      "upper limbs being partially closed, precautions. You shall conquer, for my sake.\"\r\n",
      "\r\n",
      "This I sat engaged in the chamber, down the stairs, crawling up them like a black and ominous crow. The only \r\n",
      "sound was the shadow of the Mountain. Booking \r\n",
      "down they saw that he must be about His Father's business,\r\n",
      "the service of a vast, vulgar, and meretricious beauty.\n"
     ]
    }
   ],
   "source": [
    "def genSad(letters = 500, N = 13):\n",
    "    '''\n",
    "    Generates a short, sad story.\n",
    "    \n",
    "    INPUTS:\n",
    "        letters (optional): the numbers of letters to generate before processing.\n",
    "        NOTE: This argument does not necessarily guarantee the length of the story.\n",
    "        However, it will generally be close to this.\n",
    "        N (optional): the N value used in training the data.\n",
    "        \n",
    "    OUTPUT:\n",
    "        text: a string containing the sad story.\n",
    "    '''\n",
    "    text = generate_text(sad_lm, N, letters)\n",
    "    if '.' in text:\n",
    "        while text[-1] != '.':\n",
    "            text = text[:-1]\n",
    "    return text \n",
    "print(genSad())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
